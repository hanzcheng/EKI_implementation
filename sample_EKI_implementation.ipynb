{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbcbd5c-a956-4d64-a396-02d26956b123",
   "metadata": {},
   "source": [
    "# Implementation of Ensemble Kalman Inversion (EKI) based on (Iglesias, Law, and Stewart, 2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b429dd70-409d-47f8-89ab-b7734012f377",
   "metadata": {},
   "source": [
    "This python notebook provides a step by step implementation of the Ensemble Kalman Inversion (EKI) introduced in (Iglesias, Law, and Stewart, 2013). The EKI is one numerical method for inverse problems, which aims to recover one or more parameters in a model given some measurements. The EKI algorithm consists of the following steps:\n",
    "\n",
    "1. Generating an initial ensemble (a set of initial guesses for the parameter values)\n",
    "2. Evaluating forward maps, i.e. what are the corresponding model outputs for the given parameters\n",
    "3. Computing the covariances and the Kalman gain\n",
    "4. Updating the ensemble until it satisfies a convergence criterion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00757ba0-b7e7-41b7-a43d-6a395c4c314c",
   "metadata": {},
   "source": [
    "We start by presenting two methods for generating the initial ensemble. The function initialize_uniform_ensemble considers the case where the initial ensemble is drawn from a uniform distribution with lower bound $lb$, and upper bound $ub$. On the other hand,  initialize_normal_ensemble draws the initial ensemble from a normal distribution with mean $\\mu$ and standard deviation $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cb35ed5-d07b-4481-adbd-97ca12d92799",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate the initial ensemble based on information known about the parameter's distribution\n",
    "\n",
    "# initialize ensemble based on uniform distribution\n",
    "# inputs: \n",
    "# N_ens - number of ensembles \n",
    "# N_params - number of parameters to be estimated\n",
    "# lb - lower bound\n",
    "# ub - upper bound\n",
    "# outputs:\n",
    "# ini_ens - initial set of ensembles\n",
    "# ens_mean - mean of the initial set of ensembles\n",
    "\n",
    "def initialize_uniform_ensemble(N_ens, N_params, lb = 0, ub = 1):\n",
    "    ini_ens = np.random.uniform(lb,ub,(N_ens,N_params))\n",
    "    ens_mean = np.mean(ini_ens, axis = 0)\n",
    "    return ini_ens, ens_mean\n",
    "\n",
    "# initialize ensemble based on normal distribution\n",
    "# inputs: \n",
    "# N_ens - number of ensembles \n",
    "# N_params - number of parameters to be estimated\n",
    "# mu - mean\n",
    "# sigma - standard deviation\n",
    "# outputs:\n",
    "# ini_ens - initial set of ensembles\n",
    "# ens_mean - mean of the initial set of ensembles\n",
    "def initialize_normal_ensemble(N_ens, N_params, mu = 0, sigma = 1):\n",
    "    ini_ens = np.random.normal(mu, sigma, (N_ens,N_params))\n",
    "    ens_mean = np.mean(ini_ens, axis = 0)\n",
    "    return ini_ens, ens_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c98720-0f6c-4955-b261-ae8157c6fc5d",
   "metadata": {},
   "source": [
    "The next block of code evaluates forward maps. That is, we map the parameters to the model outputs, which typically corresponds to the measurements. To illustrate this, we present two examples. The first example comes from the inverse problem of recovering the parameters $(a,b)$ for a given function\n",
    "\\begin{equation}\n",
    "f(x) = ae^{bx}, x\\in[0,1].\n",
    "\\end{equation}\n",
    "\n",
    "That is, given measurements $f(x_1),\\dots,f(x_k)$ at the data points $x_1,\\dots,x_k\\in[0,1]$, we aim to estimate the parameters $(a,b)$. Correspondingly, for the parameters $(\\hat{a},\\hat{b})$, the forward map associated to this inverse problem is given by $\\hat{f}(x) = \\hat{a}e^{\\hat{b}x}$, and returns the values $\\hat{f}(x_1),\\dots,\\hat{f}(x_k)$.\n",
    "\n",
    "The second example deals with recovering the source term $s$ of the ODE with homogeneous Dirichlet boundary conditions\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "-p'' + p &= s,\\\\\n",
    "p(0)=p(1)&=0,\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "given pointwise measurements $p(x_1),\\dots,p(x_k)$ for $x_1,\\dots,x_k \\in (0,1)$. Thus, the parameters we aim to recover are the values $s(x_1),\\dots,s(x_k)$. For this example, we consider a forward map which solves for $p$ given $s$ via the two-point flux approximation (TPFA) finite volume method. For simplicity, we work under the assumption that $x_1,\\dots,x_k$ are equally spaced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e41af5-cce0-4b42-b07a-0a6f24258849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the forward map of one parameter \n",
    "# inputs:\n",
    "# curr_ens_mem - current ensemble (set of parameters)\n",
    "# testCase - test case number\n",
    "# output:\n",
    "# forward_map_val - output of the forward model based on the current parameter set\n",
    "def evaluate_forward_map(curr_ens_mem, testCase = 1): \n",
    "    if testCase == 1:\n",
    "        forward_map_val = curr_ens_mem[0]*np.exp(curr_ens_mem[1]*x_points)\n",
    "    else:\n",
    "        if testCase == 2:\n",
    "            curr_rhs = process_rhs(curr_ens_mem, xL, xR, Nx)\n",
    "            curr_sol = np.transpose(np.linalg.solve(A_fd,curr_rhs))\n",
    "            forward_map_val = curr_sol[0][1:Nx+1]\n",
    "    return forward_map_val\n",
    "\n",
    "## compute the forward map of the entire ensemble, and also the mean of the forward mapped values\n",
    "# inputs: \n",
    "# curr_ens - current ensemble family (collection of set of parameters)\n",
    "# output:\n",
    "# curr_ens_evals - output of the forward model for the ensemble family \n",
    "def compute_ens_forward(curr_ens): \n",
    "    nDataPts = len(np.transpose(curr_ens))\n",
    "    nEnsembles = len(curr_ens)\n",
    "    curr_ens_evals = np.zeros((nEnsembles,Nx))\n",
    "    for ensNum in range(nEnsembles):\n",
    "        curr_ens_member = curr_ens[ensNum][:]\n",
    "        curr_ens_evals[ensNum][:] = evaluate_forward_map(curr_ens_member,testNum)\n",
    "    curr_ens_eval_mean = np.mean(curr_ens_evals, axis = 0)\n",
    "    return curr_ens_evals, curr_ens_eval_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125967c-b9cd-45ee-86d2-f9bb380529cc",
   "metadata": {},
   "source": [
    "The next step is to compute the covariances. As an input, it requires the current ensemble members and their corresponding forward maps. We design the function to require the means as input (alternatively, the mean may also be computed within the function itself). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e48fe4e9-f8e6-45df-8a4a-f445885f1334",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the covariance matrices\n",
    "\n",
    "# inputs:\n",
    "# curr_ens - current family of ensembles\n",
    "# curr_ens_mean - mean of curr_ens\n",
    "# curr_ens_evals - forward map (evaluations) of the current ensemble family\n",
    "# curr_ens_eval_mean - mean of curr_ens_evals\n",
    "\n",
    "# outputs:\n",
    "# CGG- sample covariance of the forward mapped data\n",
    "# CpG - sample covariance between the forward mapped data and the ensemble\n",
    "\n",
    "def compute_covariances(curr_ens,curr_ens_mean,curr_ens_evals,curr_ens_eval_mean):\n",
    "    nEnsembles = len(curr_ens)\n",
    "    nParamValues = len(np.transpose(curr_ens))\n",
    "    out_size = len(np.transpose(curr_ens_eval_mean)) #size of the output of a forward map\n",
    "    CGG = np.zeros((out_size,out_size))\n",
    "    CpG = np.zeros((nParamValues,out_size))\n",
    "    for ensNum in range(nEnsembles):\n",
    "        curr_ens_eval = curr_ens_evals[ensNum][:]\n",
    "        GG = np.add(curr_ens_eval, -1*curr_ens_eval_mean)\n",
    "        GG = [GG]\n",
    "        GGt = np.transpose(GG)\n",
    "        GGprod = np.matmul(GGt,GG)\n",
    "        CGG = np.add(CGG, GGprod)\n",
    "        curr_ens_member = curr_ens[ensNum][:]\n",
    "        pp = np.add(curr_ens_member, -1*curr_ens_mean)\n",
    "        pp = [pp]\n",
    "        pGprod = np.matmul(np.transpose(pp),GG)\n",
    "        CpG = np.add(CpG,pGprod)\n",
    "    CGG = 1/(nEnsembles-1)*CGG\n",
    "    CpG = 1/(nEnsembles-1)*CpG\n",
    "    return CGG,CpG\n",
    "\n",
    "def compute_Gamma(p_data,noise_lvl):\n",
    "    Gamma = noise_lvl*noise_lvl/4*p_data*p_data\n",
    "    Gamma = np.diag(Gamma.flatten())\n",
    "    return Gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6696491-4d70-4bcc-9207-442335a53dc2",
   "metadata": {},
   "source": [
    "After computing all the required components, we can now update the ensemble. The computed components that are needed are the current ensemble, its forward maps, and the covariance matrices. Together with these, we also require the measured data and the assumed noise level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83f9bfc2-fd06-486c-98ed-8196005f3a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## update the ensemble\n",
    "\n",
    "# inputs:\n",
    "# curr_ens - current family of ensembles\n",
    "# curr_ens_evals - forward map (evaluations) of the current ensemble family\n",
    "# CGG- sample covariance of the forward mapped data\n",
    "# CpG - sample covariance between the forward mapped data and the ensemble\n",
    "# Gamma - covariance of the data\n",
    "# p_noisy - measured data / data provided\n",
    "# noise_lvl - assumed level of noise\n",
    "\n",
    "# output:\n",
    "# new_ensemble - ensemble obtained after an EKI update\n",
    "\n",
    "def update_ensemble(curr_ens,curr_ens_evals,CGG,CpG,Gamma,p_noisy,noise_lvl):\n",
    "    nEnsembles = len(curr_ens)\n",
    "    nOutputs = len(p_noisy)\n",
    "    nParamValues = len(np.transpose(curr_ens))\n",
    "    new_ensemble = np.zeros((nEnsembles,nParamValues))\n",
    "    Kalman_lhs = np.add(CGG,Gamma)\n",
    "    for ensNum in range(nEnsembles):\n",
    "        eta_ens = np.zeros((1,nOutputs))\n",
    "        curr_ens_member = curr_ens[ensNum][:]\n",
    "        curr_ens_eval = curr_ens_evals[ensNum][:]\n",
    "        for paramNum in range(nOutputs):\n",
    "            eta_ens[0][paramNum] = np.random.normal(0,p_noisy[paramNum]*noise_lvl)\n",
    "        Kalman_rhs = np.add(np.transpose(p_noisy),eta_ens)\n",
    "        Kalman_rhs = np.add(Kalman_rhs, -1*curr_ens_eval)\n",
    "        Kalman_gain = np.linalg.solve(Kalman_lhs,np.transpose(Kalman_rhs))\n",
    "        Kalman_gain = np.matmul(CpG,Kalman_gain)\n",
    "        new_ensemble[ensNum][:] = np.add(curr_ens_member, np.transpose(Kalman_gain))\n",
    "\n",
    "    return new_ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d3f0d4-00b4-4011-bf19-f0cc40f7ce89",
   "metadata": {},
   "source": [
    "Finally, we provide the EKI algorithm. It terminates upon reaching the maximum number of iterations, or when the difference between the forward map and the data is less than the noise level. The main inputs are the number of ensembles and the maximum number of iterations. Following these, we require to specify the data points, their noise level, and which test case they came from.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba6952f0-2372-4be3-806c-34b31a042988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coḿpute the parameter values via the EKI method\n",
    "\n",
    "#inputs: \n",
    "# nEnsembles - number of ensembles\n",
    "# maxIter - maximum number of iterations / EKI updates\n",
    "# p_noisy - measured data / data provided\n",
    "# noise_lvl - assumed level of noise\n",
    "# testNum - number of the test case\n",
    "\n",
    "#outputs: \n",
    "# current_ensemble - final (updated) ensemble after EKI has converged or maxIter has been reached\n",
    "# ens_mean - mean of current_ensemble\n",
    "\n",
    "def compute_parameter_values(nEnsembles,maxIter,p_noisy,noise_lvl,testNum):\n",
    "    if testNum == 1:\n",
    "        nParams = 2\n",
    "        lb = 1\n",
    "        ub = 4\n",
    "    elif testNum == 2:\n",
    "        nParams = len(p_noisy)\n",
    "        lb = 0\n",
    "        ub = 1\n",
    "    # compute the covariance of the data\n",
    "    Gamma = compute_Gamma(p_noisy,noise_lvl)\n",
    "    # initialize the ensemble\n",
    "    ini_ens, ens_mean = initialize_uniform_ensemble(nEnsembles, nParams, lb, ub)\n",
    "    # compute the forward maps\n",
    "    [ens_evals, ens_eval_mean] =  compute_ens_forward(ini_ens)\n",
    "    # compute the covariance matrices\n",
    "    CGG,CpG = compute_covariances(ini_ens,ens_mean,ens_evals,ens_eval_mean)\n",
    "    current_ensemble = copy_matrix(ini_ens)\n",
    "    ens_mean = np.mean(current_ensemble, axis = 0)\n",
    "    numIter = 0\n",
    "    while numIter<maxIter:\n",
    "        [ens_evals, ens_eval_mean] =  compute_ens_forward(current_ensemble)\n",
    "        err_vec = np.add(ens_eval_mean,-1*np.transpose(p_noisy))\n",
    "        err_now = np.linalg.norm(np.add(ens_eval_mean,-1*np.transpose(p_noisy)))/np.linalg.norm(p_noisy)\n",
    "        if err_now<noise_lvl:\n",
    "            #print(err_now)\n",
    "            #print(numIter)\n",
    "            break\n",
    "        CGG,CpG = compute_covariances(current_ensemble,ens_mean,ens_evals,ens_eval_mean)\n",
    "        updated_ensemble =  update_ensemble(current_ensemble, ens_evals, CGG,CpG, Gamma, p_noisy, noise_lvl)\n",
    "        current_ensemble = copy_matrix(updated_ensemble)\n",
    "        ens_mean = np.mean(current_ensemble, axis = 0)\n",
    "        numIter += 1\n",
    "        if numIter == maxIter:\n",
    "            print (err_now)\n",
    "            print (\"Ensemble did not converge after maximum number of iterations\")\n",
    "    [ens_evals, ens_eval_mean] =  compute_ens_forward(current_ensemble)\n",
    "    #plt.plot(x_points,ens_eval_mean)\n",
    "    return current_ensemble, ens_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e10cf6f-f670-4de5-ae14-9fcb1c4e56e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code for copying an array or matrix in Python, so that the variable doesn't get overwritten\n",
    "def copy_matrix(my_matrix):\n",
    "    dim1 = len(my_matrix)\n",
    "    dim2 = len(np.transpose(my_matrix))\n",
    "    copied_matrix = np.zeros((dim1,dim2))\n",
    "    for i in range(dim1):\n",
    "            copied_matrix[i][:] = my_matrix[i][:]\n",
    "    return copied_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7b0bf-6d45-4f77-aa2e-ecaefbbeadab",
   "metadata": {},
   "source": [
    "Functions needed for generating test case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a750efca-cfcd-4c29-b671-f932c190b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def generate_matrix(xL=0, xR = 1, Nx = 20): #generates an (Nx+2) X (Nx+2) finite volume matrix for approximating -p''+p on (xL,xR)\n",
    "    nTotal = Nx+2 #Nx interior points, 2 boundary points\n",
    "    dx = (xR-xL)/(Nx+1)\n",
    "    A = np.zeros((nTotal,nTotal)) #initialize the (Nx+2) X (Nx+2) matrix for discretizing the problem \n",
    "    A[0][0]=1 #Dirichlet BC\n",
    "    A[Nx+1][Nx+1] = 1\n",
    "    for i in range(1,Nx+1):\n",
    "        #diffusion / second-derivative component\n",
    "        A[i][i-1] = -1/dx\n",
    "        A[i][i] = 2/dx \n",
    "        A[i][i+1] = -1/dx\n",
    "        #reaction component\n",
    "        A[i][i] = A[i][i] + dx\n",
    "    return A\n",
    "\n",
    "#function / source term for the right hand side of the PDE \n",
    "def rhs_function(x):\n",
    "    return math.sin(math.pi*x)\n",
    "\n",
    "#evaluate the function at the Nx interior points in the domain\n",
    "def generate_rhs(xL=0, xR = 1, Nx = 20, BCl=0, BCr=0):\n",
    "    nTotal = Nx+2\n",
    "    dx = (xR-xL)/(Nx+1)\n",
    "    b = np.zeros((nTotal,1))\n",
    "    b[0] = BCl\n",
    "    b[Nx+1] = BCr\n",
    "    x_points = np.linspace(xL,xR,Nx+2)\n",
    "    for i in range(1,Nx+1):\n",
    "        b[i] = rhs_function(x_points[i])\n",
    "    return b\n",
    "\n",
    "#process the value of the right hand side for a finite volume scheme (approximate an integral for a FV cell)\n",
    "def process_rhs(rhs_vals, xL=0, xR = 1, Nx = 20, BCl=0, BCr=0):\n",
    "    nTotal = Nx+2\n",
    "    dx = (xR-xL)/(Nx+1)\n",
    "    b = np.zeros((nTotal,1))\n",
    "    b[0] = BCl\n",
    "    b[Nx+1] = BCr\n",
    "    for i in range(1,Nx+1):\n",
    "        b[i] = rhs_vals[i-1]*dx\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b1378-4a63-4b21-a8f8-19902b67170d",
   "metadata": {},
   "source": [
    "# Generate data \n",
    "For our examples, we obtain the data by evaluating the models at the true parameter values. The latter block of code generates the exact data from the true parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69805188-62a6-4756-8b68-19a7e7c331ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_param_values(testCase = 1):\n",
    "    if testCase == 1:\n",
    "        true_vals = [3,2]\n",
    "    elif testCase == 2:\n",
    "        true_vals = generate_rhs(xL,xR,Nx)\n",
    "        true_vals = true_vals[1:Nx+1]        \n",
    "    return true_vals\n",
    "    \n",
    "\n",
    "def generate_data(Nx,true_vals, xL = 0, xR = 1, testCase = 1):  \n",
    "    if testCase == 1:\n",
    "        x_points = np.linspace(xL,xR,Nx)\n",
    "        p_data = true_vals[0]*np.exp(true_vals[1]*x_points)     \n",
    "        p_data = np.transpose(p_data)\n",
    "    elif testCase == 2:\n",
    "        x_points = np.linspace(xL,xR,Nx+2)\n",
    "        A_fd = generate_matrix(xL,xR,Nx) \n",
    "        b = process_rhs(true_vals,xL,xR,Nx)\n",
    "        p_data = np.linalg.solve(A_fd,b)\n",
    "        p_data = p_data[1:Nx+1]\n",
    "        x_points = x_points[1:Nx+1]\n",
    "    #plt.plot(x_points,p_data)\n",
    "    return p_data, x_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b19412-b07b-414a-9844-df960ccd6b85",
   "metadata": {},
   "source": [
    "# Adding noise to model data\n",
    "We now add noise to the data (this will be the data we try to recover RHS from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fce05cac-c4f1-4c17-82b6-4d90f6bba812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_data(p_data, noise_lvl = 1e-3):\n",
    "    Nx = len(p_data)\n",
    "    mu, sigma = 0, noise_lvl # mean and standard deviation\n",
    "    s = np.random.normal(mu, sigma, Nx) \n",
    "    p_noisy = np.zeros((Nx,1))\n",
    "    for i in range(Nx):\n",
    "        p_noisy[i] = p_data[i]*(1+s[i])\n",
    "    return p_noisy,noise_lvl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732fdd9f-2578-4e77-82b3-596eac6039cc",
   "metadata": {},
   "source": [
    "# Initializing and running the tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43311223-1447-4ddb-a85a-5b287f903089",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx = 15 #number of data points\n",
    "N_ens = 40 #number of ensembles\n",
    "maxIter = 20 #maximum number of iterations\n",
    "noise_lvl = 1e-3\n",
    "testNum = 1\n",
    "if testNum == 2:\n",
    "    xL = 0\n",
    "    xR = 1\n",
    "    A_fd = generate_matrix(xL,xR,Nx) \n",
    "true_vals = true_param_values(testNum)\n",
    "p_data, x_points = generate_data(Nx, true_vals, testCase = testNum)\n",
    "p_noisy, noise_lvl = add_noise_to_data(p_data,noise_lvl)\n",
    "\n",
    "#plt.plot(x_points,p_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55791a8e-bb63-4889-b447-de6fdb788eea",
   "metadata": {},
   "source": [
    "Run the EKI algorithm and compute the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c444cfbd-6cc7-4413-8747-2dd72607faa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00058805 0.00051718]\n",
      "0.0007831250194518339\n"
     ]
    }
   ],
   "source": [
    "converged_ens,converged_mean = compute_parameter_values(N_ens,maxIter,p_noisy,noise_lvl,testNum)\n",
    "#print(converged_mean)\n",
    "ens_err = np.add(converged_mean,-1*np.transpose(true_vals))\n",
    "ens_err = np.abs(ens_err)\n",
    "err_norm = np.linalg.norm(ens_err)\n",
    "print(ens_err)\n",
    "print(err_norm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
